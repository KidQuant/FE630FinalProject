{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "64319548-5372-4bfc-a1b5-95607f113d12",
      "metadata": {
        "id": "64319548-5372-4bfc-a1b5-95607f113d12"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import statsmodels.api as sm\n",
        "import cvxpy as cp\n",
        "from datetime import date, timedelta\n",
        "from scipy.optimize import minimize\n",
        "import warnings\n",
        "\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "483cac7a",
      "metadata": {},
      "source": [
        "#### Comment this section in if your are having rate limit issues.\n",
        "#### Otherwise, run the code as normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "ef6509ba-f8b3-4ce0-a28c-23b335f1896e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef6509ba-f8b3-4ce0-a28c-23b335f1896e",
        "outputId": "1b25bdb0-edee-4b20-d103-0ecbd0e7e615"
      },
      "outputs": [],
      "source": [
        "\n",
        "# data = pd.read_csv('data.csv')\n",
        "# data['Date'] = pd.to_datetime(data['Date'])\n",
        "# Convert the 'Date' column to datetime objects after loading the data\n",
        "etfs = ['DBA',\t'EPP',\t'EWJ',\t'FEZ',\t'FXE',\t'GLD',\t'ILF',\t'QQQ',\t'SHV',\t'SPY',\t'USO','XBI'\t]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9021400b",
      "metadata": {},
      "outputs": [],
      "source": [
        "etf_data = yf.download(etfs, start='2007-03-01', end='2024-12-31')['Close']\n",
        "\n",
        "\n",
        "etf_returns = etf_data.pct_change().dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e837d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "ff_factors = pd.read_csv('F-F_Research_Data_Factors_daily.CSV', skiprows=3, index_col=0)\n",
        "ff_factors.dropna(inplace=True)\n",
        "ff_factors.tail()\n",
        "ff_factors = ff_factors.loc['2007-03-01':]\n",
        "ff_factors.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "66bf59fe-b4c3-45bd-8b44-0b64799b286f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "66bf59fe-b4c3-45bd-8b44-0b64799b286f",
        "outputId": "fd9a3058-8a1d-4c9e-eee7-cd2c8e55e8b4"
      },
      "outputs": [],
      "source": [
        "\n",
        "etf_returns = etf_data.pct_change().dropna()\n",
        "\n",
        "# Convert ff_factors index to datetime if it's not already\n",
        "ff_factors.index = pd.to_datetime(ff_factors.index, format='%Y%m%d')\n",
        "\n",
        "# Align the data\n",
        "etf_returns = etf_returns.reindex(ff_factors.index).dropna()\n",
        "ff_factors = ff_factors.reindex(etf_returns.index).dropna()\n",
        "\n",
        "# Create merged data\n",
        "data = pd.concat([etf_returns, ff_factors], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d63ce81f",
      "metadata": {},
      "outputs": [],
      "source": [
        "data.reset_index(inplace=True)\n",
        "data.rename(columns={'index': 'Date'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c34b3873",
      "metadata": {},
      "source": [
        "#### Optimization routines "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "566a0152-e250-4dbc-beb3-89d43a07503c",
      "metadata": {
        "id": "566a0152-e250-4dbc-beb3-89d43a07503c"
      },
      "outputs": [],
      "source": [
        "def strategy_I(expected_returns, cov_matrix, betas, beta_constraints, lambd):\n",
        "    \"\"\"\n",
        "    Optimizes portfolio weights using the best aspects of previous implementations.\n",
        "\n",
        "    Parameters:\n",
        "    - expected_returns: Expected returns for each asset (1D array)\n",
        "    - cov_matrix: Covariance matrix of returns (2D array)\n",
        "    - factor_loadings: DataFrame of factor exposures (rows: assets, columns: factors)\n",
        "    - beta_col: The column name in factor_loadings to use for beta constraint (e.g., 'Mkt-RF')\n",
        "    - beta_constraints: Tuple of (min_beta, max_beta)\n",
        "    - lambd: Risk aversion parameter\n",
        "\n",
        "    Returns:\n",
        "    - Optimal portfolio weights (1D numpy array)\n",
        "    \"\"\"\n",
        "    n = len(expected_returns)\n",
        "    w = cp.Variable(n)\n",
        "    # Ensure covariance matrix is symmetric for numerical stability\n",
        "    cov_matrix = (cov_matrix + cov_matrix.T) / 2\n",
        "    # Portfolio return\n",
        "    portfolio_return = expected_returns @ w\n",
        "    # Portfolio variance\n",
        "    portfolio_risk = cp.quad_form(w, cov_matrix)\n",
        "    # Portfolio beta (generalized to any factor column)\n",
        "    portfolio_beta = betas @ w\n",
        "    # Constraints\n",
        "\n",
        "    constraints = [\n",
        "        cp.sum(w) == 1,\n",
        "        portfolio_beta >= beta_constraints[0],\n",
        "        portfolio_beta <= beta_constraints[1],\n",
        "        w >= -2,\n",
        "        w <= 2\n",
        "    ]\n",
        "    # Objective: maximize risk-adjusted return (using standard deviation)\n",
        "    objective = cp.Maximize(portfolio_return - lambd * portfolio_risk)\n",
        "    prob = cp.Problem(objective, constraints)\n",
        "    prob.solve()\n",
        "    return w.value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "3aaaa8ba-ef5e-4c2c-b906-2a9e105e24ea",
      "metadata": {
        "id": "3aaaa8ba-ef5e-4c2c-b906-2a9e105e24ea"
      },
      "outputs": [],
      "source": [
        "def strategy_II(expected_returns, returns_data, betas, beta_constraints, lambd, benchmark_returns, covar):\n",
        "    \"\"\"\n",
        "    Optimize portfolio weights using Strategy II approach\n",
        "\n",
        "    Parameters:\n",
        "    - expected_returns: Expected returns for each asset (numpy array)\n",
        "    - returns_data: Historical returns data (DataFrame)\n",
        "    - betas: calculated betas of each ETF (numpy array)\n",
        "    - beta_constraints: Tuple of (min_beta, max_beta)\n",
        "    - lambd: Risk aversion parameter\n",
        "    - benchmark_returns: Returns of benchmark portfolio (Series)\n",
        "    - covar: covariance matrix (DataFrame)\n",
        "    \"\"\"\n",
        "    n = len(expected_returns)\n",
        "\n",
        "    # Helper functions (moved inside)\n",
        "    def TEV(weights):\n",
        "        \"\"\"Tracking error volatility\"\"\"\n",
        "        portfolio_returns = returns_data @ weights\n",
        "        tracking_error = portfolio_returns - benchmark_returns\n",
        "        return np.sqrt(np.var(tracking_error))\n",
        "\n",
        "    def portRisk(weights):\n",
        "        \"\"\"Portfolio risk\"\"\"\n",
        "        return weights.T @ covar @ weights\n",
        "\n",
        "    # Find SPY index if it exists\n",
        "    spy_idx = None\n",
        "    if 'SPY' in returns_data.columns:\n",
        "        spy_idx = returns_data.columns.get_loc('SPY')\n",
        "\n",
        "    def objective(weights):\n",
        "        # Calculate portfolio return relative to benchmark\n",
        "        if spy_idx is not None:\n",
        "            port_return = np.dot(weights, expected_returns) - expected_returns[spy_idx]\n",
        "        else:\n",
        "            port_return = np.dot(weights, expected_returns) - np.mean(benchmark_returns)\n",
        "\n",
        "        return -(port_return/TEV(weights) - lambd * np.sqrt(portRisk(weights)))\n",
        "\n",
        "    # Constraints\n",
        "    constraints = [\n",
        "        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},  # Full investment\n",
        "        {'type': 'ineq', 'fun': lambda w: beta_constraints[1] - np.dot(w, betas)},  # Max beta\n",
        "        {'type': 'ineq', 'fun': lambda w: np.dot(w, betas) - beta_constraints[0]},  # Min beta\n",
        "    ]\n",
        "\n",
        "    # Optimization setup\n",
        "    result = minimize(\n",
        "        objective,\n",
        "        x0=np.ones(n)/n,  # Equal weights initial guess\n",
        "        bounds=[(-2, 2)]*n,\n",
        "        constraints=constraints,\n",
        "        method='SLSQP',\n",
        "        options={'maxiter': 1000}\n",
        "    )\n",
        "\n",
        "    return result.x if result.success else np.ones(n)/n  # Fallback to equal weights if fails"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2030eaf6",
      "metadata": {},
      "source": [
        "#### Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "29a357fb-79de-491a-8810-0eb4690e62fe",
      "metadata": {
        "id": "29a357fb-79de-491a-8810-0eb4690e62fe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def getRetCovEst(retLag, covLag, date):\n",
        "    etf_returns = data\n",
        "\n",
        "    etf_returns['Date'] = pd.to_datetime(etf_returns['Date'])\n",
        "\n",
        "    #change to grab # of obs instead of cal days\n",
        "    covRets = etf_returns[(etf_returns['Date'] <= date)].sort_values(by='Date',ascending=False).head(covLag)\n",
        "\n",
        "    cov = covRets[etfs].cov()\n",
        "    factorRets = etf_returns[etf_returns['Date'] <= date].sort_values(by='Date',ascending=False).head(retLag)\n",
        "    market_risk_premiums = pd.DataFrame(index=etf_returns.columns, columns=['Market Risk Premium'])\n",
        "    return_est = pd.DataFrame(index=etf_returns.columns, columns=['ExpectedReturn'])\n",
        "\n",
        "\n",
        "    # Run regression for each ETF to get factor exposures\n",
        "    X = sm.add_constant(factorRets[['Mkt-RF','SMB','HML']],has_constant='add')\n",
        "\n",
        "    for etf in etfs:\n",
        "        y = factorRets[[etf]]\n",
        "        # Run regression\n",
        "        model = sm.OLS(y, X).fit()\n",
        "        # Calculate market risk premium and estimate returns\n",
        "        beta1 = model.params['Mkt-RF']\n",
        "        avg_market_return = factorRets['Mkt-RF'].mean()\n",
        "        market_risk_premium = beta1 * avg_market_return\n",
        "        beta2 = model.params['SMB']\n",
        "        avg_SMB = factorRets['SMB'].mean()\n",
        "        beta3 = model.params['HML']\n",
        "        avg_HML = factorRets['HML'].mean()\n",
        "        market_risk_premiums.loc[etf, 'Market Risk Premium'] = market_risk_premium\n",
        "        expected_ret = factorRets['RF'].mean() + market_risk_premium + beta2 * avg_SMB + beta3 * avg_HML\n",
        "        return_est.loc[etf,'ExpectedReturn'] = expected_ret\n",
        "\n",
        "    #calculate betas - SPY will always have a beta of 1\n",
        "    betas = pd.DataFrame(index=etfs, columns=['Beta'])\n",
        "    for etf in etfs:\n",
        "        if etf == \"SPY\":\n",
        "            betas.loc[etf,'Beta'] = 1\n",
        "        else:\n",
        "            tickRet = factorRets[['Date',etf]]\n",
        "            spyrets = factorRets[['Date','SPY']]\n",
        "            mergedRet = pd.merge(tickRet, spyrets,how='left',on='Date')\n",
        "            tcov = mergedRet[[etf,'SPY']].cov()\n",
        "            beta = tcov.iloc[0][1]/tcov.iloc[1][1]\n",
        "            betas.loc[etf, 'Beta']= beta\n",
        "\n",
        "    return cov, betas, return_est.dropna(), factorRets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "5109fe80-2e81-494e-8d8f-7186a076e39a",
      "metadata": {
        "id": "5109fe80-2e81-494e-8d8f-7186a076e39a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def getRebalDates(ret_dates, min, max):\n",
        "    rebalDates = []\n",
        "    mondays = []\n",
        "    current_date = min\n",
        "    #get all mondays between our min and max date\n",
        "    while current_date.weekday() != 0:  # Monday is 0\n",
        "        current_date += timedelta(days=1)\n",
        "    while current_date <= max:\n",
        "        mondays.append(current_date)\n",
        "        current_date += timedelta(days=7)\n",
        "    #get each monday in our analysis period\n",
        "    for d in mondays:\n",
        "        #if monday is not in our return days (proxy for us trading days), add the tuesday\n",
        "        #we can assume that if we're missing monday it's because monday is a market holiday\n",
        "        #if monday is a market holiday, Tuesday MUST bet a trading day - markets cant be closed for 4 days in a row\n",
        "        if d in ret_dates:\n",
        "            rebalDates.append(d)\n",
        "        else:\n",
        "            rebalDates.append(d + timedelta(days=1))\n",
        "    return rebalDates\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "2708ed1e-970d-43ec-b17d-63b522a392ee",
      "metadata": {
        "id": "2708ed1e-970d-43ec-b17d-63b522a392ee"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_performance_metrics(returns):\n",
        "    \"\"\"Calculate key performance metrics from return series\"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    # Cumulative returns\n",
        "    metrics['Cumulative Return'] = (1 + returns).prod() - 1\n",
        "\n",
        "    # Daily returns\n",
        "    metrics['Daily Mean (Arith)'] = returns.mean()\n",
        "    metrics['Daily Mean (Geom)'] = ((1 + returns).prod()) ** (1/len(returns)) - 1\n",
        "    metrics['Daily Min Return'] = returns.min()\n",
        "\n",
        "    # Risk metrics\n",
        "    metrics['Volatility'] = returns.std() * np.sqrt(252)\n",
        "    metrics['Sharpe Ratio'] = metrics['Daily Mean (Geom)'] * np.sqrt(252) / metrics['Volatility']\n",
        "\n",
        "    # Drawdown\n",
        "    cum_returns = (1 + returns).cumprod()\n",
        "    peak = cum_returns.expanding().max()\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    metrics['Max Drawdown'] = drawdown.min()\n",
        "\n",
        "    # Higher moments\n",
        "    metrics['Skewness'] = returns.skew()\n",
        "    metrics['Kurtosis'] = returns.kurtosis()\n",
        "\n",
        "    # Tail risk\n",
        "    metrics['VaR (95%)'] = returns.quantile(0.05)\n",
        "    metrics['CVaR (95%)'] = returns[returns <= returns.quantile(0.05)].mean()\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5906196e",
      "metadata": {},
      "source": [
        "#### Code to run full backtest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "842c77b3-2c5b-4698-87da-067d70670c91",
      "metadata": {
        "id": "842c77b3-2c5b-4698-87da-067d70670c91"
      },
      "outputs": [],
      "source": [
        "def runBacktest(startdt, enddt, ret_est_per, cov_est_per, lambda_val):\n",
        "    \"\"\"\n",
        "    Run backtest for both strategies between start and end dates\n",
        "\n",
        "    Parameters:\n",
        "    - startdt: Start date (YYYY-MM-DD)\n",
        "    - enddt: End date (YYYY-MM-DD)\n",
        "    - ret_est_per: Returns estimation period (days)\n",
        "    - cov_est_per: Covariance estimation period (days)\n",
        "    - lambda_val: Risk aversion parameter\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary with weights, returns, and portfolio values\n",
        "    \"\"\"\n",
        "    # Convert and validate dates\n",
        "    startdt = pd.to_datetime(startdt)\n",
        "    enddt = pd.to_datetime(enddt)\n",
        "\n",
        "    # Filter data for backtest period\n",
        "    period_data = data[(data['Date'] >= startdt) & (data['Date'] <= enddt)].copy()\n",
        "\n",
        "    # Initialize results DataFrames with proper datetime index\n",
        "    results = {\n",
        "        'weights_I': pd.DataFrame(index=period_data['Date'].unique(), columns=etfs),\n",
        "        'weights_II': pd.DataFrame(index=period_data['Date'].unique(), columns=etfs),\n",
        "        'daily_returns': pd.DataFrame(index=period_data['Date'], columns=['Strategy_I', 'Strategy_II', 'SPY']),\n",
        "        'portfolio_values': pd.DataFrame(index=period_data['Date'], columns=['Strategy_I', 'Strategy_II', 'SPY'])\n",
        "    }\n",
        "\n",
        "    # Initialize portfolio values\n",
        "    results['portfolio_values'].iloc[0] = 100\n",
        "\n",
        "    # Get rebalancing dates (weekly)\n",
        "    rebalDates = getRebalDates(period_data['Date'].values, startdt, enddt)\n",
        "\n",
        "    for i, rebal_date in enumerate(rebalDates):\n",
        "        try:\n",
        "            # Get estimation data\n",
        "            cov, betas, retest, inputRets = getRetCovEst(ret_est_per, cov_est_per, rebal_date)\n",
        "\n",
        "            # Optimize portfolios\n",
        "            weights_I = strategy_I(\n",
        "                retest['ExpectedReturn'].values,\n",
        "                cov,\n",
        "                betas['Beta'].values,\n",
        "                [-0.5, 0.5],\n",
        "                lambda_val\n",
        "            )\n",
        "\n",
        "            weights_II = strategy_II(\n",
        "                retest['ExpectedReturn'].values,\n",
        "                inputRets[etfs],\n",
        "                betas['Beta'].values,\n",
        "                [-2, 2],\n",
        "                lambda_val,\n",
        "                inputRets['SPY'],\n",
        "                cov\n",
        "            )\n",
        "\n",
        "            # Store weights\n",
        "            results['weights_I'].loc[rebal_date] = weights_I\n",
        "            results['weights_II'].loc[rebal_date] = weights_II\n",
        "\n",
        "            # Calculate period returns\n",
        "            next_rebal = rebalDates[i+1] if i < len(rebalDates)-1 else enddt\n",
        "            period_mask = (period_data['Date'] >= rebal_date) & (period_data['Date'] <= next_rebal)\n",
        "            period_returns = period_data.loc[period_mask]\n",
        "\n",
        "            # Calculate daily returns and portfolio values            \n",
        "            for j in range(len(period_returns)):\n",
        "                date = period_returns.iloc[j]['Date']\n",
        "                daily_ret = period_returns.iloc[j][etfs].values\n",
        "\n",
        "                # Calculate strategy returns\n",
        "                ret_I = np.dot(weights_I, daily_ret)\n",
        "                ret_II = np.dot(weights_II, daily_ret)\n",
        "                # Store returns\n",
        "                results['daily_returns'].loc[date, 'Strategy_I'] = ret_I\n",
        "                results['daily_returns'].loc[date, 'Strategy_II'] = ret_II\n",
        "                results['daily_returns'].loc[date, 'SPY'] = period_returns.iloc[j]['SPY']\n",
        "\n",
        "                # Update portfolio values\n",
        "                if j == 0 and i == 0:\n",
        "                    prev_value_I = 100\n",
        "                    prev_value_II = 100\n",
        "                    prev_value_spy = 100\n",
        "                else:\n",
        "                    if j != 0:\n",
        "                        prev_date = period_returns.iloc[j-1]['Date']\n",
        "                    else:\n",
        "                        prev_date = period_returns.iloc[0]['Date']\n",
        "                    prev_value_I = results['portfolio_values'].loc[prev_date, 'Strategy_I']\n",
        "                    prev_value_II = results['portfolio_values'].loc[prev_date, 'Strategy_II']\n",
        "                    prev_value_spy = results['portfolio_values'].loc[prev_date, 'SPY']\n",
        "\n",
        "                results['portfolio_values'].loc[date, 'Strategy_I'] = prev_value_I * (1 + ret_I)\n",
        "                results['portfolio_values'].loc[date, 'Strategy_II'] = prev_value_II * (1 + ret_II)\n",
        "                results['portfolio_values'].loc[date, 'SPY'] = prev_value_spy * (1 + period_returns.iloc[j]['SPY'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {rebal_date}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Clean up results\n",
        "    results['weights_I'] = results['weights_I'].dropna(how='all')\n",
        "    results['weights_II'] = results['weights_II'].dropna(how='all')\n",
        "    results['daily_returns'] = results['daily_returns'].dropna()\n",
        "    results['portfolio_values'] = results['portfolio_values'].dropna()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e27d1b7-c4ca-451b-a6f6-9b4e4fabc67f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e27d1b7-c4ca-451b-a6f6-9b4e4fabc67f",
        "outputId": "0d0dd9ff-a6c6-4230-fc82-c72c5ce13598"
      },
      "outputs": [],
      "source": [
        "#test backtest method\n",
        "t = runBacktest('2008-09-01', '2008-9-20',40,60,1)\n",
        "t"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef1a6e9",
      "metadata": {},
      "source": [
        "### Full implementation - Run each regime, for each estimation period and each Lambda value \n",
        "#### Note: this takes ~2 hours to run, please use the \"sample\" method below to verify functionality \n",
        "#### the full period and bull market runs take particularly long - the 9 variations for the pre-subprie takes about 2 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R_vf5rKbnNZt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_vf5rKbnNZt",
        "outputId": "98c14837-2c45-4a6d-ac37-e7979f0eb47a"
      },
      "outputs": [],
      "source": [
        "# Define sub-periods\n",
        "periods = {\n",
        "    'Subprime Crisis (Before)': ('2007-03-01', '2008-08-31'),\n",
        "    'Subprime Crisis (During)': ('2008-09-01', '2009-03-31'),\n",
        "    '2010s Bull Market': ('2009-04-01', '2019-12-31'),\n",
        "    'COVID (During)': ('2020-01-01', '2021-12-31'),\n",
        "    'COVID (After)': ('2022-01-01', '2024-12-31'), # Assuming end of 2024 as per requirement\n",
        "    'Whole Period': ('2007-03-01', '2024-12-31')\n",
        "}\n",
        "\n",
        "# Define lambda values\n",
        "lambdas = [0.1, 0.5, 1]\n",
        "\n",
        "# Define estimation periods\n",
        "estimation_periods = [(40, 60), (90, 60),(180,60)] # (ret_est_per, cov_est_per) representing S40/60 and S90/60\n",
        "\n",
        "# Create a table to store results\n",
        "performance_table = pd.DataFrame()\n",
        "ret_dict = {}\n",
        "pnl_dict = {}\n",
        "keys = []\n",
        "\n",
        "for p in periods:\n",
        "    period_name = p\n",
        "    start_date, end_date = periods[period_name]\n",
        "    for l in lambdas:\n",
        "        lambda_val = l # lambda = 0.1\n",
        "        for estp in estimation_periods:\n",
        "            ret_per, cov_per = estp # S40/60\n",
        "            \n",
        "            key = p + \"|\" + str(l) +\"|\"+ str(ret_per)\n",
        "            keys.append(key)\n",
        "\n",
        "            print(f\"Running backtest for: {period_name}, Estimation: {ret_per}/{cov_per}, Lambda: {lambda_val}\")\n",
        "\n",
        "            try:\n",
        "                backtest_results = runBacktest(start_date, end_date, ret_per, cov_per, lambda_val)\n",
        "\n",
        "                # Calculate performance metrics\n",
        "                metrics_I = calculate_performance_metrics(backtest_results['daily_returns']['Strategy_I'])\n",
        "                metrics_II = calculate_performance_metrics(backtest_results['daily_returns']['Strategy_II'])\n",
        "                metrics_SPY = calculate_performance_metrics(backtest_results['daily_returns']['SPY'])\n",
        "                ret_dict[key]=backtest_results['daily_returns']\n",
        "                pnl_dict[key]=backtest_results['portfolio_values']\n",
        "                # Add metrics to the table\n",
        "                temp_df = pd.DataFrame({\n",
        "                    'Period': [period_name, period_name, period_name],\n",
        "                    'Estimation': [f'S{ret_per}/{cov_per}', f'S{ret_per}/{cov_per}', f'S{ret_per}/{cov_per}'],\n",
        "                    'Lambda': [lambda_val, lambda_val, lambda_val],\n",
        "                    'Strategy': ['Strategy I', 'Strategy II', 'SPY'],\n",
        "                    'Cumulative Return': [metrics_I['Cumulative Return'], metrics_II['Cumulative Return'], metrics_SPY['Cumulative Return']],\n",
        "                    'Annualized Volatility': [metrics_I['Volatility'], metrics_II['Volatility'], metrics_SPY['Volatility']],\n",
        "                    'Sharpe Ratio': [metrics_I['Sharpe Ratio'], metrics_II['Sharpe Ratio'], metrics_SPY['Sharpe Ratio']],\n",
        "                    'Max Drawdown': [metrics_I['Max Drawdown'], metrics_II['Max Drawdown'], metrics_SPY['Max Drawdown']],\n",
        "                    'Skewness': [metrics_I['Skewness'], metrics_II['Skewness'], metrics_SPY['Skewness']],\n",
        "                    'Kurtosis': [metrics_I['Kurtosis'], metrics_II['Kurtosis'], metrics_SPY['Kurtosis']],\n",
        "                    'VaR (95%)': [metrics_I['VaR (95%)'], metrics_II['VaR (95%)'], metrics_SPY['VaR (95%)']],\n",
        "                    'CVaR (95%)': [metrics_I['CVaR (95%)'], metrics_II['CVaR (95%)'], metrics_SPY['CVaR (95%)']]\n",
        "                })\n",
        "\n",
        "                performance_table = pd.concat([performance_table, temp_df], ignore_index=True)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Backtest failed for {period_name}: {e}\")\n",
        "\n",
        "# Display the resulting table for the first run\n",
        "print(\"\\nPerformance Table (First Period, First Estimation, First Lambda):\")\n",
        "print(performance_table.to_markdown(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "fc86ff51",
      "metadata": {},
      "outputs": [],
      "source": [
        "#output results to csv for easier viz, analysis \n",
        "performance_table.to_csv(\"All Results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6295199c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for p in keys:\n",
        "    data = pnl_dict[p]\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.plot(data['SPY'],label='SPY')\n",
        "    plt.plot(data['Strategy_I'],label='Strat 1')\n",
        "    plt.plot(data['Strategy_II'],label='Strat 2')\n",
        "    plt.title('pnl ' + p)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c8108c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for p in keys:\n",
        "    data = pnl_dict[p]\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.hist(data['SPY'], bins=20, alpha=0.7, label='SPY')\n",
        "    plt.hist(data['Strategy_I'], bins=20, alpha=0.7, label='Strat 1')\n",
        "    plt.hist(data['Strategy_II'], bins=20, alpha=0.7, label='Strat 2')\n",
        "    plt.title('Daily Ret Dis ' + p)\n",
        "    plt.xlabel('Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a7065ca",
      "metadata": {},
      "source": [
        "#### Sample method for logic verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add97667",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sub-periods\n",
        "periods = {\n",
        "    'Subprime Crisis (Before)': ('2007-03-01', '2008-08-31')\n",
        "    # 'Subprime Crisis (During)': ('2008-09-01', '2009-03-31'),\n",
        "    # '2010s Bull Market': ('2009-04-01', '2019-12-31'),\n",
        "    # 'COVID (During)': ('2020-01-01', '2021-12-31'),\n",
        "    # 'COVID (After)': ('2022-01-01', '2024-12-31'), # Assuming end of 2024 as per requirement\n",
        "    # 'Whole Period': ('2007-03-01', '2024-12-31')\n",
        "}\n",
        "\n",
        "# Define lambda values\n",
        "lambdas = [0.1, 0.5, 1]\n",
        "\n",
        "# Define estimation periods\n",
        "estimation_periods = [(40, 60), (90, 60),(180,60)] # (ret_est_per, cov_est_per) representing S40/60 and S90/60\n",
        "\n",
        "# Create a table to store results\n",
        "performance_table_s = pd.DataFrame()\n",
        "ret_dict_s = {}\n",
        "pnl_dict_s = {}\n",
        "keys_s = []\n",
        "\n",
        "for p in periods:\n",
        "    period_name = p\n",
        "    start_date, end_date = periods[period_name]\n",
        "    for l in lambdas:\n",
        "        lambda_val = l # lambda = 0.1\n",
        "        for estp in estimation_periods:\n",
        "            ret_per, cov_per = estp # S40/60\n",
        "            \n",
        "            key = p + \"|\" + str(l) +\"|\"+ str(ret_per)\n",
        "            keys_s.append(key)\n",
        "\n",
        "            print(f\"Running backtest for: {period_name}, Estimation: {ret_per}/{cov_per}, Lambda: {lambda_val}\")\n",
        "\n",
        "            try:\n",
        "                backtest_results = runBacktest(start_date, end_date, ret_per, cov_per, lambda_val)\n",
        "\n",
        "                # Calculate performance metrics\n",
        "                metrics_I = calculate_performance_metrics(backtest_results['daily_returns']['Strategy_I'])\n",
        "                metrics_II = calculate_performance_metrics(backtest_results['daily_returns']['Strategy_II'])\n",
        "                metrics_SPY = calculate_performance_metrics(backtest_results['daily_returns']['SPY'])\n",
        "                ret_dict_s[key]=backtest_results['daily_returns']\n",
        "                pnl_dict_s[key]=backtest_results['portfolio_values']\n",
        "                # Add metrics to the table\n",
        "                temp_df = pd.DataFrame({\n",
        "                    'Period': [period_name, period_name, period_name],\n",
        "                    'Estimation': [f'S{ret_per}/{cov_per}', f'S{ret_per}/{cov_per}', f'S{ret_per}/{cov_per}'],\n",
        "                    'Lambda': [lambda_val, lambda_val, lambda_val],\n",
        "                    'Strategy': ['Strategy I', 'Strategy II', 'SPY'],\n",
        "                    'Cumulative Return': [metrics_I['Cumulative Return'], metrics_II['Cumulative Return'], metrics_SPY['Cumulative Return']],\n",
        "                    'Annualized Volatility': [metrics_I['Volatility'], metrics_II['Volatility'], metrics_SPY['Volatility']],\n",
        "                    'Sharpe Ratio': [metrics_I['Sharpe Ratio'], metrics_II['Sharpe Ratio'], metrics_SPY['Sharpe Ratio']],\n",
        "                    'Max Drawdown': [metrics_I['Max Drawdown'], metrics_II['Max Drawdown'], metrics_SPY['Max Drawdown']],\n",
        "                    'Skewness': [metrics_I['Skewness'], metrics_II['Skewness'], metrics_SPY['Skewness']],\n",
        "                    'Kurtosis': [metrics_I['Kurtosis'], metrics_II['Kurtosis'], metrics_SPY['Kurtosis']],\n",
        "                    'VaR (95%)': [metrics_I['VaR (95%)'], metrics_II['VaR (95%)'], metrics_SPY['VaR (95%)']],\n",
        "                    'CVaR (95%)': [metrics_I['CVaR (95%)'], metrics_II['CVaR (95%)'], metrics_SPY['CVaR (95%)']]\n",
        "                })\n",
        "\n",
        "                performance_table_s = pd.concat([performance_table_s, temp_df], ignore_index=True)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Backtest failed for {period_name}: {e}\")\n",
        "\n",
        "# Display the resulting table for the first run\n",
        "print(\"\\nPerformance Table (First Period, First Estimation, First Lambda):\")\n",
        "print(performance_table_s.to_markdown(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "FE630Project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
